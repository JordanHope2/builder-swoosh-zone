name: Nightly Job Scraping

on:
  workflow_dispatch:
  schedule:
    - cron: '0 2 * * *' # Runs at 2 AM UTC every day

jobs:
  scrape:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout repository
      uses: actions/checkout@v5

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.12'

    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-

    - name: Install dependencies
      run: |
        pip install -r job_scraper/requirements.txt

    - name: Run scrapers
      env:
        SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
        SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
        ADZUNA_APP_ID: ${{ secrets.ADZUNA_APP_ID }}
        ADZUNA_API_KEY: ${{ secrets.ADZUNA_API_KEY }}
      run: |
        echo "Running all scrapers via main.py..."
        python3 job_scraper/main.py
        echo "Scraping complete."

    - name: Generate run summary
      run: |
        # This is a placeholder for generating a real summary.
        # A real implementation would get the counts from the scraper run.
        echo '{
          "sources_run": ["SwissDevJobs.ch"],
          "total_extracted": 50,
          "total_inserted": 50,
          "total_deduped": 0,
          "error_rate": "0.0%"
        }' > run_summary.json
        cat run_summary.json
